{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T02:21:05.386722Z",
     "start_time": "2024-12-18T02:21:02.346764Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from config.Experiment_Config import ExperimentConfig\n",
    "from exp.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de83d11d69e94a7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T02:21:05.456823Z",
     "start_time": "2024-12-18T02:21:05.455037Z"
    }
   },
   "outputs": [],
   "source": [
    "config = ExperimentConfig.get_default_config()\n",
    "config.model_settings.embedding_type = 'XLM_roberta_large'\n",
    "config.training_settings.num_epochs = 1000\n",
    "config.training_settings.batch_size = 32\n",
    "config.tokenizer_settings.max_length = 256\n",
    "config.model_settings.weight_init = 'kaiming_normal'\n",
    "config.data_settings.imbalanced_strategy = 'weighted_sampler'\n",
    "config.data_settings.alpha = 0.1\n",
    "config.model_settings.activation = 'gelu'\n",
    "config.model_settings.fine_tune_embedding = True\n",
    "config.training_settings.early_stopping_patience = 100\n",
    "config.training_settings.task_type = 'Multi'\n",
    "config.model_settings.num_layers = 12\n",
    "config.model_settings.custom_hidden_dims = [1024, 1024, 512, 512, 256, 256, 128, 128, 64, 64, 64, 64]\n",
    "config.model_settings.use_res_net = True\n",
    "config.model_settings.use_attention = False\n",
    "config.training_settings.gradient_clip = 1.0\n",
    "config.training_settings.continue_training = True\n",
    "config.training_settings.learning_rate = 2e-5\n",
    "config.model_settings.fine_tune_lr = 5e-5\n",
    "config.model_settings.use_layer_norm = True\n",
    "config.model_settings.fine_tune_embedding = True\n",
    "config.model_settings.attention_temperature = 1.0\n",
    "config.model_settings.attention_positions = ['embedding', 'inter_lstm']\n",
    "config.model_settings.use_attention = True\n",
    "config.model_settings.fine_tune_loading_strategies = ['periodic', 'plateau']\n",
    "config.model_settings.fine_tune_mode = 'gradual'\n",
    "config.model_settings.num_frozen_layers = 12\n",
    "config.model_settings.fine_tune_reload_freq = 20\n",
    "config.model_settings.dropout_rate = 0.2\n",
    "config.model_settings.gradual_unfreeze_epochs = 50\n",
    "config.model_settings.bidirectional = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e548caf5142ff5f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T02:21:12.313680Z",
     "start_time": "2024-12-17T19:32:22.655999Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1024 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Gradual fine-tuning initialized (starting with last layer)\n",
      "📊 Trainable parameters: 7,087,872 / 109,482,240 (6.5%)\n",
      "🔄 Periodic reload triggered at epoch 0\n",
      "✅ Loaded best model weights\n",
      "\n",
      "==================================================\n",
      "EXPERIMENT CONFIGURATION\n",
      "==================================================\n",
      "\n",
      "📊 Experiment Name: BL_XL_L12H256M256_W2e-5FT5e-5CE-GeLU_WS_ATN-EI_LD-PL20_ARNTG\n",
      "📁 Model Directory: saved_models\\Multi\\question\\BL_XL_L12H256M256_W2e-5FT5e-5CE-GeLU_WS_ATN-EI_LD-PL20_ARNTG\n",
      "📈 Results Directory: results\\Multi\\question\\BL_XL_L12H256M256_W2e-5FT5e-5CE-GeLU_WS_ATN-EI_LD-PL20_ARNTG\n",
      "\n",
      "🔧 Model Configuration:\n",
      "- Model Type: BiLSTM\n",
      "- Embedding Type: XLM_roberta_large\n",
      "- Architecture:\n",
      "  • Hidden Dimensions: [1024, 1024, 512, 512, 256, 256, 128, 128, 64, 64, 64, 64]\n",
      "  • Number of Layers: 12\n",
      "  • Bidirectional: True\n",
      "  • Dropout Rate: 0.2\n",
      "\n",
      "🎯 Attention Configuration:\n",
      "- Use Attention: True\n",
      "  • Number of Heads: 8\n",
      "  • Attention Positions: ['embedding', 'inter_lstm']\n",
      "  • Attention Dropout: 0.1\n",
      "  • Temperature: 1.0\n",
      "\n",
      "🏗️ Architecture Features:\n",
      "- Residual Connections: True\n",
      "  • Residual Dropout: 0.1\n",
      "- Layer Normalization: True\n",
      "  • Layer Norm Epsilon: 1e-05\n",
      "  • Elementwise: False\n",
      "  • Affine Transform: True\n",
      "\n",
      "🔄 Fine-tuning Configuration:\n",
      "- Fine-tune Embedding: True\n",
      "  • Mode: gradual\n",
      "  • Learning Rate: 5e-05\n",
      "  • Loading Strategies: ['periodic', 'plateau']\n",
      "  • Reload Frequency: 20\n",
      "  • Plateau Patience: 5\n",
      "  • Plateau Threshold: 0.0001\n",
      "  • LR Decay Factor: 0.95\n",
      "\n",
      "⚙️ Training Configuration:\n",
      "- Batch Size: 32\n",
      "- Max Length: 256\n",
      "- Learning Rate: 2e-05\n",
      "- Weight Decay: 0.01\n",
      "- Num Epochs: 1000\n",
      "- Loss Function: cross_entropy\n",
      "- Optimizer: adamw\n",
      "- Gradient Clipping: 1.0\n",
      "\n",
      "📈 Training Control:\n",
      "- Early Stopping Patience: 100\n",
      "- Scheduler Patience: 3\n",
      "- Scheduler Factor: 0.1\n",
      "- Min Learning Rate: 1e-06\n",
      "- Checkpoint Frequency: 10\n",
      "\n",
      "📊 Data Configuration:\n",
      "- Task Type: Multi\n",
      "- Imbalanced Strategy: weighted_sampler\n",
      "  • Alpha: 1.0\n",
      "- Number of Classes: 38\n",
      "\n",
      "📚 Dataset Information:\n",
      "- Training samples: 4079\n",
      "- Validation samples: 511\n",
      "- Test samples: 510\n",
      "- Number of batches (train): 127\n",
      "\n",
      "💻 Hardware Configuration:\n",
      "- Device: cuda\n",
      "- MPS available: False\n",
      "- CUDA available: True\n",
      "- CUDA devices: 1\n",
      "\n",
      "==================================================\n",
      "\n",
      "⚠️ No valid checkpoints found, starting fresh training\n",
      "⚠️ No checkpoint found, starting training from scratch\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "⏳ Epoch 1/1000\n",
      "──────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/127 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "Training:   1%|          | 1/127 [00:00<01:44,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Training Dynamics Monitor (Epoch 1):\n",
      "⚠️ Vanishing gradient in embedding_model.encoder.layer.11.attention.self.key.bias: 0.0000\n",
      "LSTM - Mean Grad: 6.85e-05, Max Norm: 5.40e-02\n",
      "FC - Mean Grad: 3.60e-04, Max Norm: 5.36e-02\n",
      "ATTENTION - Mean Grad: 1.09e-04, Max Norm: 7.71e-02\n",
      "Activations first_layer:\n",
      "  Mean: 0.7749, Std: 1.0001\n",
      "Activations last_layer:\n",
      "  Mean: 0.0263, Std: 0.0378\n",
      "⚠️ Low activation values in last_layer\n",
      "Outputs - Mean: 0.0263, Std: 0.0378\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 127/127 [00:27<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Train Metrics (Epoch 1)\n",
      "═════════════════════════════════════════════════════════════════\n",
      "│ Accuracy     │ 0.0551       │\n",
      "─────────────────────────────────────────────────────────────────\n",
      "│ F1 Macro     │ 0.0255       │ F1 Micro     │ 0.0551       │\n",
      "─────────────────────────────────────────────────────────────────\n",
      "│ Prec Macro   │ 0.0418       │ Prec Micro   │ 0.0551       │\n",
      "─────────────────────────────────────────────────────────────────\n",
      "│ Recall Macro │ 0.0534       │ Recall Micro │ 0.0551       │\n",
      "═════════════════════════════════════════════════════════════════\n",
      "⚠️ No latest fine-tuned model found for val evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating (val): 100%|██████████| 15/15 [00:02<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Val Metrics (Epoch 1)\n",
      "═════════════════════════════════════════════════════════════════\n",
      "│ Accuracy     │ 0.0854       │\n",
      "─────────────────────────────────────────────────────────────────\n",
      "│ F1 Macro     │ 0.0461       │ F1 Micro     │ 0.0854       │\n",
      "─────────────────────────────────────────────────────────────────\n",
      "│ Prec Macro   │ 0.0667       │ Prec Micro   │ 0.0854       │\n",
      "─────────────────────────────────────────────────────────────────\n",
      "│ Recall Macro │ 0.1099       │ Recall Micro │ 0.0854       │\n",
      "═════════════════════════════════════════════════════════════════\n",
      "💾 Saved latest training plots for epoch 1\n",
      "💾 Saved latest validation plots for epoch 1\n",
      "💾 Saved latest metrics at epoch 1\n",
      "💾 Saved latest fine-tuned embedding at epoch 1\n",
      "💾 Saved latest model at epoch 1\n",
      "🏆 Saved best model at epoch 1\n",
      "🏆 Saved best training plots for epoch 1\n",
      "🏆 Saved best validation plots for epoch 1\n",
      "🏆 Saved best metrics at epoch 1\n",
      "🏆 Saved best fine-tuned embedding at epoch 1\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "⏳ Epoch 2/1000\n",
      "──────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 1/127 [00:00<00:46,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Training Dynamics Monitor (Epoch 2):\n",
      "⚠️ Vanishing gradient in embedding_model.encoder.layer.11.attention.self.key.bias: 0.0000\n",
      "LSTM - Mean Grad: 2.03e-04, Max Norm: 1.46e-01\n",
      "FC - Mean Grad: 9.19e-04, Max Norm: 1.77e-01\n",
      "ATTENTION - Mean Grad: 3.76e-04, Max Norm: 2.73e-01\n",
      "Activations first_layer:\n",
      "  Mean: 0.7733, Std: 1.0003\n",
      "Activations last_layer:\n",
      "  Mean: 0.0263, Std: 0.0718\n",
      "⚠️ Low activation values in last_layer\n",
      "Outputs - Mean: 0.0263, Std: 0.0718\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 127/127 [00:31<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Train Metrics (Epoch 2)\n",
      "═════════════════════════════════════════════════════════════════\n",
      "│ Accuracy     │ 0.2121       │\n",
      "─────────────────────────────────────────────────────────────────\n",
      "│ F1 Macro     │ 0.1117       │ F1 Micro     │ 0.2121       │\n",
      "─────────────────────────────────────────────────────────────────\n",
      "│ Prec Macro   │ 0.1406       │ Prec Micro   │ 0.2121       │\n",
      "─────────────────────────────────────────────────────────────────\n",
      "│ Recall Macro │ 0.2147       │ Recall Micro │ 0.2121       │\n",
      "═════════════════════════════════════════════════════════════════\n",
      "✅ Loaded latest fine-tuned embedding for val evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating (val): 100%|██████████| 15/15 [00:02<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Val Metrics (Epoch 2)\n",
      "═════════════════════════════════════════════════════════════════\n",
      "│ Accuracy     │ 0.2729       │\n",
      "─────────────────────────────────────────────────────────────────\n",
      "│ F1 Macro     │ 0.0797       │ F1 Micro     │ 0.2729       │\n",
      "─────────────────────────────────────────────────────────────────\n",
      "│ Prec Macro   │ 0.0839       │ Prec Micro   │ 0.2729       │\n",
      "─────────────────────────────────────────────────────────────────\n",
      "│ Recall Macro │ 0.1528       │ Recall Micro │ 0.2729       │\n",
      "═════════════════════════════════════════════════════════════════\n",
      "💾 Saved latest training plots for epoch 2\n",
      "💾 Saved latest validation plots for epoch 2\n",
      "💾 Saved latest metrics at epoch 2\n",
      "💾 Saved latest fine-tuned embedding at epoch 2\n",
      "💾 Saved latest model at epoch 2\n",
      "🏆 Saved best model at epoch 2\n",
      "🏆 Saved best training plots for epoch 2\n",
      "🏆 Saved best validation plots for epoch 2\n",
      "🏆 Saved best metrics at epoch 2\n",
      "🏆 Saved best fine-tuned embedding at epoch 2\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "⏳ Epoch 3/1000\n",
      "──────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 1/127 [00:00<01:06,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Training Dynamics Monitor (Epoch 3):\n",
      "⚠️ Vanishing gradient in embedding_model.encoder.layer.11.attention.self.key.bias: 0.0000\n",
      "LSTM - Mean Grad: 1.65e-04, Max Norm: 1.42e-01\n",
      "FC - Mean Grad: 6.01e-04, Max Norm: 1.22e-01\n",
      "ATTENTION - Mean Grad: 2.97e-04, Max Norm: 2.48e-01\n",
      "Activations first_layer:\n",
      "  Mean: 0.7723, Std: 1.0014\n",
      "Activations last_layer:\n",
      "  Mean: 0.0263, Std: 0.0893\n",
      "⚠️ Low activation values in last_layer\n",
      "Outputs - Mean: 0.0263, Std: 0.0893\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 127/127 [00:30<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Train Metrics (Epoch 3)\n",
      "═════════════════════════════════════════════════════════════════\n",
      "│ Accuracy     │ 0.2790       │\n",
      "─────────────────────────────────────────────────────────────────\n",
      "│ F1 Macro     │ 0.1890       │ F1 Micro     │ 0.2790       │\n",
      "─────────────────────────────────────────────────────────────────\n",
      "│ Prec Macro   │ 0.2202       │ Prec Micro   │ 0.2790       │\n",
      "─────────────────────────────────────────────────────────────────\n",
      "│ Recall Macro │ 0.2828       │ Recall Micro │ 0.2790       │\n",
      "═════════════════════════════════════════════════════════════════\n",
      "✅ Loaded latest fine-tuned embedding for val evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating (val): 100%|██████████| 15/15 [00:02<00:00,  6.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Val Metrics (Epoch 3)\n",
      "═════════════════════════════════════════════════════════════════\n",
      "│ Accuracy     │ 0.4146       │\n",
      "─────────────────────────────────────────────────────────────────\n",
      "│ F1 Macro     │ 0.1067       │ F1 Micro     │ 0.4146       │\n",
      "─────────────────────────────────────────────────────────────────\n",
      "│ Prec Macro   │ 0.1458       │ Prec Micro   │ 0.4146       │\n",
      "─────────────────────────────────────────────────────────────────\n",
      "│ Recall Macro │ 0.1754       │ Recall Micro │ 0.4146       │\n",
      "═════════════════════════════════════════════════════════════════\n",
      "💾 Saved latest training plots for epoch 3\n",
      "💾 Saved latest validation plots for epoch 3\n",
      "💾 Saved latest metrics at epoch 3\n",
      "💾 Saved latest fine-tuned embedding at epoch 3\n",
      "💾 Saved latest model at epoch 3\n",
      "🏆 Saved best model at epoch 3\n",
      "🏆 Saved best training plots for epoch 3\n",
      "🏆 Saved best validation plots for epoch 3\n",
      "🏆 Saved best metrics at epoch 3\n",
      "🏆 Saved best fine-tuned embedding at epoch 3\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Train model\n",
    "metrics_history = trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Multi_turn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
