{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T02:21:05.386722Z",
     "start_time": "2024-12-18T02:21:02.346764Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yunkaigao/opt/anaconda3/envs/multi_turn/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from config.Experiment_Config import ExperimentConfig\n",
    "from exp.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de83d11d69e94a7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T02:21:05.456823Z",
     "start_time": "2024-12-18T02:21:05.455037Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing checkpoint at epoch 30/100. Training will continue from the latest checkpoint.\n"
     ]
    }
   ],
   "source": [
    "config = ExperimentConfig.get_default_config()\n",
    "config.tokenizer_settings.max_length = 512\n",
    "config.training_settings.batch_size = 32\n",
    "config.data_settings.imbalanced_strategy = 'weighted_sampler'\n",
    "config.data_settings.weighted_sampler_alpha = 0.5\n",
    "config.model_settings.weight_init = 'kaiming_normal'\n",
    "config.model_settings.init_hidden_dim = 384\n",
    "config.model_settings.activation = 'gelu'\n",
    "config.model_settings.fine_tune_embedding = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e548caf5142ff5f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T02:21:12.313680Z",
     "start_time": "2024-12-17T19:32:22.655999Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1024 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EXPERIMENT CONFIGURATION\n",
      "==================================================\n",
      "\n",
      "üìä Experiment Name: BiLSTM_BERT_base_uncased_512_2_cross_entropy_kaiming_normal_weighted_sampler\n",
      "üìÅ Model Directory: saved_models/BiLSTM_BERT_base_uncased_512_2_cross_entropy_kaiming_normal_weighted_sampler\n",
      "üìà Results Directory: results/BiLSTM_BERT_base_uncased_512_2_cross_entropy_kaiming_normal_weighted_sampler\n",
      "\n",
      "üîß Model Configuration:\n",
      "- Model Type: BiLSTM\n",
      "- Embedding Type: BERT_base_uncased\n",
      "- Hidden Dimensions: [256, 128]\n",
      "- Bidirectional: True\n",
      "- Dropout Rate: 0.1\n",
      "- Attention: False\n",
      "- Fine-tune Embedding: True\n",
      "- Weight Init: kaiming_normal\n",
      "- Activation: gelu\n",
      "\n",
      "‚öôÔ∏è Training Configuration:\n",
      "- Batch Size: 32\n",
      "- Max Length: 512\n",
      "- Learning Rate: 0.001\n",
      "- Weight Decay: 0.01\n",
      "- Num Epochs: 100\n",
      "- Loss Function: cross_entropy\n",
      "- Optimizer: adam\n",
      "- Gradient Clipping: 1.0\n",
      "- Early Stopping Patience: 5\n",
      "- Scheduler Patience: 3\n",
      "- Scheduler Factor: 0.1\n",
      "- Min Learning Rate: 1e-06\n",
      "- Checkpoint Frequency: 10\n",
      "\n",
      "üìä Data Configuration:\n",
      "- Imbalanced Strategy: weighted_sampler\n",
      "  ‚Ä¢ Alpha: 0.5\n",
      "- Number of Classes: 38\n",
      "- Class Names: ['Academic Cheating', 'Animal Abuse', 'Art Forgery', 'Consumer Fraud', 'Copyright Issues', 'Cybercrime', 'DIY Medical Treatments', 'Digital Piracy', 'Discriminatory Behavior', 'Drugs', 'Economic Crime', 'Elder Abuse', 'Endangering National Security', 'Endangering Public Health', 'Food Safety Violations', 'Human Trafficking', 'Illegal Dumping', 'Illegal Logging', 'Insulting Behavior', 'Labor Exploitation', 'Medical Malpractice', 'Mental Manipulation', 'Overfishing', 'Patent Infringement', 'Perjury', 'Physical Harm', 'Privacy Violation', 'Psychological Harm', 'Public Nuisance', 'Self Harm', 'Sexual Content', 'Soil Contamination', 'Tax Evasion', 'Trespassing on Critical Infrastructure', 'Violence', 'White-Collar Crime', 'Wildlife Poaching', 'safe']\n",
      "\n",
      "üìÇ Directory Configuration:\n",
      "- Model Save Dir: saved_models\n",
      "- Results Save Dir: results\n",
      "- Fine-tuned Models Dir: fine_tuned_models\n",
      "\n",
      "üíª Hardware Configuration:\n",
      "- Device: cpu\n",
      "- MPS available: True\n",
      "- CUDA available: False\n",
      "\n",
      "üìö Dataset Information:\n",
      "- Training samples: 4079\n",
      "- Validation samples: 511\n",
      "- Test samples: 510\n",
      "- Number of batches (train): 127\n",
      "\n",
      "==================================================\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Resuming training from epoch 5/100\n",
      "Continuing training from epoch 5\n",
      "Using device: cpu\n",
      "MPS (Apple Silicon) available: True\n",
      "MPS (Apple Silicon) built: True\n",
      "CUDA available: False\n",
      "Number of CUDA devices: 0\n",
      "Experiment name: BiLSTM_BERT_base_uncased_512_2_cross_entropy_kaiming_normal_weighted_sampler\n",
      "\n",
      "Epoch 5/100 - BiLSTM_BERT_base_uncased_512_2_cross_entropy_kaiming_normal_weighted_sampler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "Training:   0%|          | 0/127 [00:00<?, ?it/s]TOKENIZERS_PARALLELISM=(true | false)\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 127/127 [1:01:28<00:00, 29.04s/it]\n",
      "/Users/yunkaigao/opt/anaconda3/envs/multi_turn/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Evaluating (val): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:57<00:00,  7.86s/it]\n",
      "/Users/yunkaigao/opt/anaconda3/envs/multi_turn/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Results:\n",
      "\n",
      "Training Metrics:\n",
      "accuracy: 0.1558\n",
      "precision_macro: 0.0041\n",
      "precision_micro: 0.1558\n",
      "recall_macro: 0.0263\n",
      "recall_micro: 0.1558\n",
      "f1_macro: 0.0071\n",
      "f1_micro: 0.1558\n",
      "loss: 3.5282\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.4958\n",
      "precision_macro: 0.0130\n",
      "precision_micro: 0.4958\n",
      "recall_macro: 0.0263\n",
      "recall_micro: 0.4958\n",
      "f1_macro: 0.0174\n",
      "f1_micro: 0.4958\n",
      "loss: 3.1937\n",
      "\n",
      "Epoch 6/100 - BiLSTM_BERT_base_uncased_512_2_cross_entropy_kaiming_normal_weighted_sampler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 127/127 [1:01:42<00:00, 29.16s/it]\n",
      "/Users/yunkaigao/opt/anaconda3/envs/multi_turn/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Evaluating (val): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:57<00:00,  7.86s/it]\n",
      "/Users/yunkaigao/opt/anaconda3/envs/multi_turn/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Results:\n",
      "\n",
      "Training Metrics:\n",
      "accuracy: 0.1511\n",
      "precision_macro: 0.0040\n",
      "precision_micro: 0.1511\n",
      "recall_macro: 0.0263\n",
      "recall_micro: 0.1511\n",
      "f1_macro: 0.0069\n",
      "f1_micro: 0.1511\n",
      "loss: 3.5328\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.4958\n",
      "precision_macro: 0.0130\n",
      "precision_micro: 0.4958\n",
      "recall_macro: 0.0263\n",
      "recall_micro: 0.4958\n",
      "f1_macro: 0.0174\n",
      "f1_micro: 0.4958\n",
      "loss: 3.1971\n",
      "\n",
      "Epoch 7/100 - BiLSTM_BERT_base_uncased_512_2_cross_entropy_kaiming_normal_weighted_sampler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 127/127 [1:01:51<00:00, 29.23s/it]\n",
      "/Users/yunkaigao/opt/anaconda3/envs/multi_turn/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Evaluating (val): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:58<00:00,  7.90s/it]\n",
      "/Users/yunkaigao/opt/anaconda3/envs/multi_turn/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Results:\n",
      "\n",
      "Training Metrics:\n",
      "accuracy: 0.1415\n",
      "precision_macro: 0.0037\n",
      "precision_micro: 0.1415\n",
      "recall_macro: 0.0263\n",
      "recall_micro: 0.1415\n",
      "f1_macro: 0.0065\n",
      "f1_micro: 0.1415\n",
      "loss: 3.5425\n",
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.4958\n",
      "precision_macro: 0.0130\n",
      "precision_micro: 0.4958\n",
      "recall_macro: 0.0263\n",
      "recall_micro: 0.4958\n",
      "f1_macro: 0.0174\n",
      "f1_micro: 0.4958\n",
      "loss: 3.1882\n",
      "\n",
      "Epoch 8/100 - BiLSTM_BERT_base_uncased_512_2_cross_entropy_kaiming_normal_weighted_sampler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 113/127 [55:19<06:56, 29.76s/it]"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"Starting training...\")\n",
    "metrics_history = trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
